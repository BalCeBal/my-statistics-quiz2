concept,true_statement,false_statement,justification
Maximum Likelihood Estimation,Maximum likelihood estimators can often be obtained analytically by setting the derivative of the log-likelihood to zero.,Maximum likelihood estimators can always be found in closed form regardless of model complexity.,Closed-form solutions for MLE exist only under certain models with tractable likelihoods; complex models often require numerical optimization.
Bootstrap,The bootstrap technique can be used to construct confidence intervals.,The bootstrap always yields exact confidence interval coverage for any sample size.,"Bootstrap provides approximate coverage that improves with larger samples, but does not guarantee exact coverage in finite samples."
Fisher Information,The Fisher information measures the amount of information a sample carries about an unknown parameter.,The Fisher information is equal to the square root of the likelihood function.,Fisher information is defined as the expected value of the squared score or the negative expected second derivative of the log-likelihood.
Admissible Estimator,An admissible estimator has no other estimator with uniformly lower risk.,An admissible estimator is always better than any other estimator.,"Admissibility means no estimator dominates it in risk across all parameter values, but it may perform worse in some regions."
t-Distribution,Degrees of freedom of the t-distribution correspond to the sample size minus one when estimating variance.,The degrees of freedom of the t-distribution tell us to what degree we are free to use the data.,"For a sample of size n, the t-distribution uses n-1 degrees of freedom due to estimating the population variance."
Bias-Variance Tradeoff,The mean squared error of an estimator can be computed as the sum of the squared bias and the variance.,The mean squared error of an estimator is the product of the squared bias and the variance.,"By definition, MSE = Bias^2 + Variance, not their product."
Maximum Likelihood Estimation,The MLE maximizes the likelihood function over the parameter space.,The MLE (maximum likelihood estimator) cannot be larger than the maximum of the likelihood function.,"The MLE is the parameter value that yields the highest likelihood, not constrained by the maximum likelihood value itself."
Score Confidence Interval,The score confidence interval for sample proportions is used in particular when the sample size is large.,The score interval for proportions is preferred when sample sizes are small because it relies on the exact binomial distribution.,"The Wilson (score) interval uses normal approximation, which is accurate for larger samples but not exact for small n."
Conjugate Priors,"If a posterior distribution belongs to the same family as the prior, then the prior is called conjugate.",A prior is conjugate if it assigns equal probability to all parameter values.,A flat prior is noninformative; conjugacy refers to posterior and prior belonging to the same distribution family.
Noninformative Priors,Jeffreys prior is invariant under reparameterization.,Jeffrey's prior is proportional to the square of the Fisher information.,"Jeffreys prior is the square root of the determinant of the Fisher information matrix, giving invariance under transformations."
Bayesian Estimation,Bayesian estimation aims to obtain the so-called posterior distribution.,Bayesian estimation only uses prior distributions and ignores the likelihood from data.,Bayesian inference combines prior beliefs with data via the likelihood to produce the posterior distribution.
Maximum Likelihood Estimation,"If we have a family of probability distributions with unknown parameters, a maximum likelihood estimator often provides a good way to estimate these parameters.",A maximum likelihood estimator always produces unbiased estimates in small samples.,MLEs are asymptotically unbiased and efficient but can be biased in finite samples.
Maximum A Posteriori Estimation,"To obtain a point estimator, one sometimes takes the mode of the posterior distribution. (This is called the MAP estimator.)",The MAP estimator is the mean of the posterior distribution.,The MAP is the posterior mode; the posterior mean is a different point estimator.
Fisher Information,"The larger the Fisher information, the smaller the variance of the corresponding MLE.","The larger the Fisher information, the larger the variance of the corresponding MLE.","Asymptotically, Var(θ̂) ≈ 1/I(θ), so greater Fisher information leads to smaller estimator variance."
Maximum Likelihood Estimation,"If we have a family of probability distributions with unknown parameters, a maximum likelihood estimator provides often a good way to estimate these parameters.",Maximum likelihood estimators are always unbiased regardless of sample size.,"While MLE is asymptotically unbiased, finite-sample bias can occur for many models."
Admissible Estimator,An admissible estimator cannot be uniformly improved upon by any other estimator.,An admissible estimator is always better than any other estimator.,"Admissibility means no estimator has strictly lower risk for all parameter values, though performance may vary."
Pivot,The known distribution of pivots is often used to construct confidence intervals.,"Pivot distributions typically depend on unknown parameters, so they cannot be used directly for confidence intervals.","By definition, a pivot has a distribution independent of parameters, making it ideal for confidence interval construction."
Bias-Variance Tradeoff,The mean squared error of an estimator can be computed as the sum of the squared bias and the variance.,The mean squared error of an estimator is the sum of its bias and variance.,"MSE = E[(θ̂−θ)^2] = Bias(θ̂)^2 + Var(θ̂), not Bias + Variance."
Credible and HPD Intervals,"If the posterior is symmetric and unimodal, then the credible and HPD intervals are identical.","If the posterior is skewed, credible and HPD intervals are identical.",Central credible intervals coincide with HPD only for symmetric unimodal posteriors; skewness yields different intervals.
Bayesian Informativeness,"If a posterior distribution is equal to the prior, then the data are reasonably uninformative.","If the likelihood is constant, the posterior will be narrower than the prior.","When the data provide no information (constant likelihood), the posterior equals the prior; it does not become tighter."
Jeffreys' Prior,Jeffrey’s prior is proportional to the square-root of the Fisher information.,Jeffrey’s prior is proportional to the Fisher information.,"Jeffreys prior is defined as π(θ) ∝ √I(θ), not directly proportional to I(θ)."
Confidence Intervals for Proportions,The score confidence interval for proportions is generally more accurate than the Wald interval.,The Wald confidence interval for proportions is usually more accurate than the score confidence interval.,"Research shows the Wilson (score) interval has better coverage properties than the Wald interval, especially for small samples."
Simple Random Sampling,A simple random sample of n subjects from a population is one in which each individual has an equal chance of being selected and selections are independent.,"In a simple random sample, subjects are selected based on a predetermined characteristic rather than randomly.","By definition, simple random sampling gives all individuals equal probability and independence in selection."
Sampling Distribution,"A sampling distribution is the probability distribution of a statistic (e.g., the sample mean) over all possible samples of a given size.",A sampling distribution describes the distribution of the data points within a single sample.,"Sampling distributions characterize the variability of a statistic across repeated samples, not the data within one sample."
Binomial Distribution,The binomial distribution models the number of successes in n independent Bernoulli trials with success probability p.,The binomial distribution is used for continuous measurements of time between events.,"The binomial is discrete and counts successes, whereas continuous time between events is modeled by distributions like exponential."
Hypergeometric Distribution,The hypergeometric distribution models successes without replacement from a finite population.,The hypergeometric distribution assumes sampling with replacement.,"Hypergeometric applies to sampling without replacement, leading to dependent trials."
Standard Error,The standard error of the sample mean equals the population standard deviation divided by the square root of n.,The standard error of the sample mean increases as the sample size increases.,"SE = σ / √n decreases as n increases, reflecting more precision in larger samples."
Law of Large Numbers,"The law of large numbers states that as the sample size grows, the sample mean converges in probability to the population mean.",The law of large numbers guarantees exact equality between sample mean and population mean for any sample size.,"LLN ensures convergence in probability as n→∞, not equality in finite samples."
Chebyshev's Inequality,"Chebyshev's inequality gives an upper bound on the probability that a random variable deviates from its mean by a specified amount, regardless of its distribution.",Chebyshev's inequality only applies to normally distributed variables.,"Chebyshev's bound holds for any distribution with finite variance, not just the normal."
Central Limit Theorem,"According to the central limit theorem, the distribution of the standardized sample mean approaches a standard normal distribution as sample size increases.",The central limit theorem ensures that the original data become normally distributed as sample size increases.,"CLT refers to the sampling distribution of statistics (like the mean), not the raw data distribution."
Delta Method,The delta method uses a first-order Taylor series expansion to approximate the sampling distribution of a function of an estimator.,The delta method provides an exact distribution for any transformation of an estimator.,"Delta method is an approximation via linearization, not an exact result except in special cases."
Gamma Distribution,"The gamma distribution is a continuous distribution characterized by shape and rate (or scale) parameters, often used to model positive continuous data.",The gamma distribution is discrete and counts integer outcomes.,"Gamma is continuous on (0,∞), described by density f(x)=λ^k x^{k−1} e^{−λx}/Γ(k)."
Unbiased Estimator,An estimator θ̂ is unbiased if its expected value equals the true parameter value.,An estimator θ̂ is unbiased if its variance equals zero.,"Unbiasedness means E[θ̂] = θ, regardless of the estimator's variance."
Consistency,An estimator is consistent if it converges in probability to the true parameter as the sample size increases.,An estimator is consistent if it has zero bias in finite samples.,"Consistency refers to convergence of θ̂ to θ in probability as n→∞, not bias in small samples."
Mean Squared Error,The mean squared error of an estimator decomposes as the sum of its variance and the square of its bias.,The mean squared error of an estimator equals the product of its variance and bias.,MSE(θ̂) = E[(θ̂−θ)²] = Var(θ̂) + [Bias(θ̂)]² by definition.
Likelihood Function,The likelihood function is the joint density of the observed data viewed as a function of the parameter.,The likelihood function is a probability distribution over parameter values that integrates to one.,Likelihoods are not normalized over θ; they represent data probability for each parameter value.
Maximum Likelihood Estimation,The MLE of a parameter is the value that maximizes the likelihood function given the observed data.,The MLE is the average of all possible parameter values weighted by their likelihood.,"MLE finds the parameter value where ℓ(θ) attains its maximum, not a weighted average."
Properties of MLE,"Under regularity conditions, the MLE is asymptotically normal with variance equal to the inverse Fisher information.",The MLE has an exact normal distribution for any finite sample size.,"Asymptotic normality holds as n→∞: √n(θ̂−θ) → N(0, I(θ)⁻¹), but finite-sample distribution may differ."
Fisher Information,Fisher information measures the expected curvature of the log-likelihood and is inversely related to estimator variance.,Fisher information equals the variance of the parameter estimate.,"I(θ) = −E[∂²logℓ(θ)/∂θ²]; asymptotically Var(θ̂) ≈ 1/[nI(θ)], not equal to I(θ)."
Sufficient Statistic,A sufficient statistic summarizes all information in the data about a parameter without loss.,A sufficient statistic depends on unknown parameters to be computed from the data.,"By definition, sufficiency implies the likelihood factorizes, so the statistic alone carries all info."
Wald Confidence Interval,The Wald confidence interval for a proportion uses the normal approximation of the standardized estimator θ̂ ± z_{1−α/2} SE(θ̂).,The Wald confidence interval for a proportion is exact regardless of sample size.,Wald intervals rely on approximate normality and can be inaccurate for small samples or extreme p̂.
Bootstrap Bias Correction,Bootstrap IV involves estimating the bias from bootstrap samples and adjusting the estimator before computing intervals.,Bootstrap IV creates confidence intervals by fitting a parametric distribution to bootstrap estimates.,"Bootstrap bias correction subtracts the average bootstrap bias from θ̂, then uses the adjusted value in interval formulas."
